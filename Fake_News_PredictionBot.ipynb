{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWVdrXT-6V81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa088f4b-f6c9-4d1a-e310-acb0918a0ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OHSwsA-nVtW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qeCooU7nXz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb559a9-5c4d-4e53-ff78-8c85bb616d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 42841 rows and 5 columns.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # Import Pandas\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Project/new_db.csv\"  # Update this based on your Drive path\n",
        "df = pd.read_csv(file_path)  # Read the CSV file\n",
        "print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a72Zj3TynZCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834552c4-8fa3-40fe-dfd4-dc88ca8e10f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch_geometric)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, torch_geometric, nvidia-cusolver-cu12, datasets, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric-2.6.1 torchtext-0.18.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torchtext torch_geometric datasets nltk pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRfLhf7cn0KI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c053fc2a-a46c-4362-e7fc-2fafafc249ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Vb-9ukoUf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02824874-ef5c-4cc5-be22-64f410d2b0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMxEPSXaoZlF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import spacy\n",
        "import contractions\n",
        "import nltk\n",
        "from transformers import RobertaTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources (if not already available)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English stopwords as a set for faster lookups\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/Project/new_db.csv\"  # Update if needed\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# Keep necessary columns\n",
        "columns_to_keep = df[[\"subject\", \"date\", \"label\"]]\n",
        "\n",
        "# Combine title and text\n",
        "df[\"Statement\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
        "\n",
        "# Drop unnecessary columns before processing\n",
        "df.drop(columns=[\"title\", \"text\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "print(f\"Dataset after dropping missing values: {df.shape[0]} rows.\")\n",
        "\n",
        "# Load spaCy model (disable unnecessary components for speed)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# **Fast text cleaning using spaCy pipe (batch processing)**\n",
        "def clean_texts(texts):\n",
        "    texts = [contractions.fix(text.lower()) for text in texts]  # Expand contractions & lowercase\n",
        "    texts = [re.sub(r'\\W+', ' ', text).strip() for text in texts]  # Remove punctuation & extra spaces\n",
        "\n",
        "    docs = nlp.pipe(texts, batch_size=50)  # Process in batches\n",
        "    return [\" \".join([token.lemma_ for token in doc if token.text not in stop_words]) for doc in docs]\n",
        "\n",
        "print(\"Applying text cleaning...\")\n",
        "df[\"Statement\"] = clean_texts(df[\"Statement\"].tolist())\n",
        "print(\"Text cleaning complete.\")\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# **Fast batch tokenization**\n",
        "print(\"Tokenizing text...\")\n",
        "df[\"tokenized\"] = tokenizer.batch_encode_plus(\n",
        "    df[\"Statement\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128\n",
        ")[\"input_ids\"]\n",
        "print(\"Tokenization complete.\")\n",
        "\n",
        "# Convert labels to numeric (Fake = 0, Real = 1)\n",
        "df[\"label\"] = df[\"label\"].astype(\"category\").cat.codes\n",
        "\n",
        "# Ensure correct column order\n",
        "df = df[[\"subject\", \"date\", \"label\", \"Statement\", \"tokenized\"]]\n",
        "\n",
        "# Save preprocessed dataset\n",
        "output_file = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Preprocessing complete. File saved as '{output_file}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OkElPt9paKe"
      },
      "outputs": [],
      "source": [
        "total_rows = df.shape[0]\n",
        "print(\"Total rows in new_db.csv:\", total_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anqya2IoobSP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the final preprocessed dataset (Update the correct path if needed)\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if the 'label' column exists\n",
        "if \"label\" in df.columns:\n",
        "    # Count the number of Real (1) and Fake (0) news articles\n",
        "    label_counts = df[\"label\"].value_counts()\n",
        "    print(\"Label distribution:\\n\", label_counts)\n",
        "else:\n",
        "    print(\"Error: 'label' column not found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPKWhQDSpPzL"
      },
      "outputs": [],
      "source": [
        "total_rows = df.shape[0]\n",
        "print(\"Total rows in final_preprocessed_news.csv:\", total_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LGCUcnkpxqd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# ✅ Load the processed dataset\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Update if needed\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# ✅ Print the first 5 rows to check\n",
        "print(\"✅ Dataset Loaded Successfully!\")\n",
        "print(df_processed.head())  # Show the first 5 rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdE1tS2Qoa95"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast  # To safely convert string representation of list to actual list\n",
        "import torch\n",
        "\n",
        "# Load the preprocessed dataset (Update the path if running in Colab)\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Adjust if needed\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure the tokenized column exists\n",
        "if \"tokenized\" not in df_processed.columns:\n",
        "    raise ValueError(\"The 'tokenized' column is missing in the dataset!\")\n",
        "\n",
        "# Drop rows where \"tokenized\" is NaN (to prevent errors)\n",
        "df_processed = df_processed.dropna(subset=[\"tokenized\"])\n",
        "\n",
        "# Convert the tokenized column from string representation to actual lists\n",
        "def safe_convert(text):\n",
        "    try:\n",
        "        return torch.tensor(ast.literal_eval(text)) if pd.notna(text) else torch.tensor([])\n",
        "    except:\n",
        "        return torch.tensor([])\n",
        "\n",
        "df_processed[\"tokenized\"] = df_processed[\"tokenized\"].apply(safe_convert)\n",
        "\n",
        "# Filter out empty tensors before stacking\n",
        "valid_tensors = [t for t in df_processed[\"tokenized\"] if t.numel() > 0]\n",
        "\n",
        "if len(valid_tensors) > 0:\n",
        "    input_tensors = torch.stack(valid_tensors)\n",
        "    print(\"Tokenized data converted to PyTorch tensor shape:\", input_tensors.shape)\n",
        "else:\n",
        "    print(\"Error: No valid tokenized tensors found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfFeekEkpvkh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Check if valid_tensors exist before stacking\n",
        "if len(valid_tensors) > 0:\n",
        "    input_tensors = torch.stack(valid_tensors)\n",
        "\n",
        "    # Define the save path (Google Drive if in Colab, or current directory)\n",
        "    save_path = \"/content/drive/MyDrive/Project/tokenized_data.pt\"  # Adjust as needed\n",
        "\n",
        "    # Save the tensor data to a file\n",
        "    torch.save(input_tensors, save_path)\n",
        "\n",
        "    print(f\"Tokenized data saved as '{save_path}'\")\n",
        "else:\n",
        "    print(\"Error: No valid tokenized tensors to save!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x5OKjzVqCv6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load the preprocessed dataset (Update path if needed)\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Update if running in Colab\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure 'label' column is numeric (convert & handle errors)\n",
        "df_processed[\"label\"] = pd.to_numeric(df_processed[\"label\"], errors=\"coerce\")  # Converts invalid values to NaN\n",
        "\n",
        "# Drop rows with NaN labels (if any)\n",
        "df_processed = df_processed.dropna(subset=[\"label\"])\n",
        "\n",
        "# Convert labels to integer\n",
        "df_processed[\"label\"] = df_processed[\"label\"].astype(int)\n",
        "\n",
        "# Load tokenized tensors safely\n",
        "tokenized_path = \"/content/drive/MyDrive/Project/tokenized_data.pt\"  # Update if needed\n",
        "tokenized_tensors = torch.load(tokenized_path)  # Load PyTorch tensor file\n",
        "\n",
        "# Ensure the dataset size matches\n",
        "num_samples = min(len(df_processed), len(tokenized_tensors))\n",
        "df_processed = df_processed.iloc[:num_samples]  # Trim dataset if needed\n",
        "tokenized_tensors = tokenized_tensors[:num_samples]  # Trim tokenized data if needed\n",
        "\n",
        "# Convert labels into PyTorch tensor\n",
        "labels_tensor = torch.tensor(df_processed[\"label\"].tolist(), dtype=torch.long)\n",
        "\n",
        "# Define Dataset class\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, tokenized_data, labels):\n",
        "        self.tokenized_data = tokenized_data\n",
        "        self.labels = labels  # Ensure labels are also tensors\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokenized_data[idx], self.labels[idx]\n",
        "\n",
        "# Create dataset\n",
        "news_dataset = NewsDataset(tokenized_tensors, labels_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(news_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Debugging info\n",
        "print(f\"✅ DataLoader is ready with batch size {batch_size}\")\n",
        "print(f\"Total samples in dataset: {len(news_dataset)}\")\n",
        "print(f\"Unique labels count: {df_processed['label'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfHcN4aEqHNL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "\n",
        "class RoBERTa_GNN_HAN(nn.Module):\n",
        "    def __init__(self, roberta_model_name=\"roberta-base\", hidden_dim=128, num_classes=2):\n",
        "        super(RoBERTa_GNN_HAN, self).__init__()\n",
        "\n",
        "        # Load pre-trained RoBERTa\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
        "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_dim)\n",
        "\n",
        "        # GNN Layer - Modeling \"Statement vs Subject\" Relationship\n",
        "        self.gnn = nn.Linear(hidden_dim, hidden_dim)  # Replace with actual GNN implementation\n",
        "\n",
        "        # HAN Layer - Attention over Subject Context\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Output Layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subject_embedding):\n",
        "        \"\"\"\n",
        "        input_ids: Tokenized input text (statements)\n",
        "        attention_mask: Attention mask for RoBERTa\n",
        "        subject_embedding: Encoded subject information (additional input for GNN)\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: RoBERTa for Statement Encoding\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = roberta_output.last_hidden_state[:, 0, :]  # CLS token representation\n",
        "\n",
        "        # Step 2: Feature Extraction\n",
        "        x = self.relu(self.fc1(hidden_states))\n",
        "\n",
        "        # Step 3: GNN Layer - Incorporate Subject Information\n",
        "        x = self.relu(self.gnn(x + subject_embedding))  # Incorporate subject data into GNN\n",
        "\n",
        "        # Step 4: HAN - Attention Mechanism\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = x * attention_weights  # Apply attention\n",
        "\n",
        "        # Step 5: Classification\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAG65xEUqN4L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the preprocessed dataset (Update path if needed)\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Update if needed\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure labels are numeric and properly formatted\n",
        "df_processed[\"label\"] = pd.to_numeric(df_processed[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# Load tokenized data safely\n",
        "tokenized_path = \"/content/drive/MyDrive/Project/tokenized_data.pt\"  # Update if needed\n",
        "try:\n",
        "    tokenized_tensors = torch.load(tokenized_path)  # Load PyTorch tensor file\n",
        "except FileNotFoundError:\n",
        "    raise ValueError(f\"Error: File '{tokenized_path}' not found!\")\n",
        "\n",
        "# Ensure dataset and tensor alignment\n",
        "num_samples = min(len(df_processed), len(tokenized_tensors))\n",
        "df_processed = df_processed.iloc[:num_samples]  # Trim dataset if needed\n",
        "tokenized_tensors = tokenized_tensors[:num_samples]  # Trim tokenized data if needed\n",
        "\n",
        "# Convert labels into tensor format\n",
        "labels_tensor = torch.tensor(df_processed[\"label\"].tolist(), dtype=torch.long)\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = TensorDataset(tokenized_tensors, labels_tensor)\n",
        "\n",
        "# Define DataLoader\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Debugging Info\n",
        "print(f\"✅ DataLoader ready with batch size {batch_size}\")\n",
        "print(f\"Total samples in dataset: {len(train_dataset)}\")\n",
        "print(f\"Unique labels count: {df_processed['label'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djb6_17UrryL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Load the processed dataset\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Update if needed\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# ✅ Load tokenized data\n",
        "tokenized_path = \"/content/drive/MyDrive/Project/tokenized_data.pt\"\n",
        "tokenized_tensors = torch.load(tokenized_path)\n",
        "\n",
        "# ✅ Convert labels to tensors\n",
        "labels_tensor = torch.tensor(df_processed[\"label\"].values, dtype=torch.long)\n",
        "\n",
        "# ✅ Ensure dataset and labels match\n",
        "num_samples = min(len(tokenized_tensors), len(labels_tensor))\n",
        "tokenized_tensors = tokenized_tensors[:num_samples]\n",
        "labels_tensor = labels_tensor[:num_samples]\n",
        "\n",
        "# ✅ Split into Train & Test (80% Train, 20% Test)\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokenized_tensors, labels_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Create TensorDataset\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "\n",
        "# ✅ Define DataLoader (MUST BE CREATED)\n",
        "batch_size = 16  # Change as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✅ DataLoader ready! Train: {len(train_dataset)} samples, Test: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMuVFkuD4gBF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import os\n",
        "from torch.amp import autocast, GradScaler  # ✅ Updated for PyTorch 2.1+\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from google.colab import drive  # ✅ Google Drive support\n",
        "\n",
        "# ✅ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Define save paths in Google Drive\n",
        "save_dir = \"/content/drive/MyDrive/Project/Checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(save_dir, \"best_model.pt\")  # ✅ Save the best model\n",
        "training_log_path = os.path.join(save_dir, \"training_log.txt\")  # ✅ Save training logs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RoBERTa_GNN_HAN().to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)  # ✅ L2 Regularization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "scaler = GradScaler(\"cuda\")  # ✅ Updated Syntax\n",
        "\n",
        "# ✅ Function to save model checkpoint\n",
        "def save_checkpoint(epoch, model, optimizer, loss, path):\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"loss\": loss\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"📌 Checkpoint saved at epoch {epoch+1} ✅\")\n",
        "\n",
        "# ✅ Ensure train_loader is defined\n",
        "if 'train_loader' not in globals():\n",
        "    raise ValueError(\"🚨 Error: `train_loader` is not defined!\")\n",
        "\n",
        "# ✅ Training loop (NO Early Stopping)\n",
        "num_epochs = 10  # Train for full epochs\n",
        "start_time = time.time()\n",
        "training_log = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for step, batch in enumerate(train_loader, 1):\n",
        "        if batch is None:\n",
        "            continue  # ✅ Skip empty batches\n",
        "\n",
        "        if len(batch) == 2:  # Handle missing attention_mask\n",
        "            input_ids, labels = batch\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "            attention_mask = torch.ones_like(input_ids, device=device)  # ✅ Ensure a valid tensor\n",
        "            subject_embedding = torch.zeros_like(input_ids, device=device)  # ✅ Dummy subject embedding (replace later)\n",
        "        else:\n",
        "            input_ids, attention_mask, subject_embedding, labels = batch\n",
        "            input_ids, attention_mask, subject_embedding, labels = (\n",
        "                input_ids.to(device), attention_mask.to(device), subject_embedding.to(device), labels.to(device)\n",
        "            )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(\"cuda\"):  # ✅ Updated Mixed Precision Syntax\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, subject_embedding=subject_embedding)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)  # ✅ Prevent Exploding Gradients\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    epoch_time = (time.time() - epoch_start_time) / 60  # ✅ Convert to minutes\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    log_entry = f\"\\n🔹 Epoch {epoch+1}/{num_epochs} | Avg Loss: {avg_loss:.4f} | Time: {epoch_time:.2f} min\"\n",
        "    training_log.append(log_entry)\n",
        "    print(log_entry)\n",
        "\n",
        "    # ✅ Save model at every epoch (NO early stopping)\n",
        "    save_checkpoint(epoch, model, optimizer, avg_loss, best_model_path)\n",
        "\n",
        "total_time = (time.time() - start_time) / 60  # ✅ Convert to minutes\n",
        "final_log = f\"\\n🎯 Training complete in {total_time:.2f} minutes! 🚀\"\n",
        "training_log.append(final_log)\n",
        "print(final_log)\n",
        "\n",
        "# ✅ Save Training Log to Google Drive\n",
        "with open(training_log_path, \"w\") as log_file:\n",
        "    log_file.writelines(\"\\n\".join(training_log))\n",
        "\n",
        "print(f\"\\n📄 Training log saved at: {training_log_path}\")\n",
        "print(f\"\\n✅ Best model saved at: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z59HMn4Z5pqZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ✅ Split into train and test sets (80% Train, 20% Test)\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokenized_tensors, labels_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Create TensorDataset\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "\n",
        "# ✅ Define DataLoaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"✅ DataLoader ready! Train: {len(train_dataset)} samples, Test: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7iW0r0J4iRf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Load the preprocessed dataset\n",
        "file_path = \"/content/drive/MyDrive/Project/final_preprocessed_news.csv\"  # Update path if needed\n",
        "df_processed = pd.read_csv(file_path)\n",
        "\n",
        "# ✅ Ensure 'label' column exists\n",
        "if \"label\" not in df_processed.columns:\n",
        "    raise KeyError(\"🚨 Error: 'label' column not found in df_processed! Available columns: \", df_processed.columns)\n",
        "\n",
        "# ✅ Load tokenized data\n",
        "tokenized_path = \"/content/drive/MyDrive/Project/tokenized_data.pt\"\n",
        "tokenized_tensors = torch.load(tokenized_path)\n",
        "\n",
        "# ✅ Convert labels to tensors\n",
        "labels_tensor = torch.tensor(df_processed[\"label\"].values, dtype=torch.long)\n",
        "\n",
        "# ✅ Print Debugging Information\n",
        "print(f\"🔍 Tokenized data samples: {len(tokenized_tensors)}\")\n",
        "print(f\"🔍 Label samples: {len(labels_tensor)}\")\n",
        "\n",
        "# ✅ Fix dataset size mismatch\n",
        "num_samples = min(len(tokenized_tensors), len(labels_tensor))\n",
        "tokenized_tensors = tokenized_tensors[:num_samples]\n",
        "labels_tensor = labels_tensor[:num_samples]\n",
        "\n",
        "print(f\"✅ Fixed dataset sizes! New size: {num_samples} samples\")\n",
        "\n",
        "# ✅ Split into train and test sets (80% train, 20% test)\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokenized_tensors, labels_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Create TensorDataset\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "\n",
        "# ✅ Define DataLoaders (Optimized for Speed)\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"✅ DataLoader ready! Train: {len(train_dataset)} samples, Test: {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StUzqchA5bPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ✅ Define device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Load the trained model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"  # Update path if needed\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# ✅ Initialize the model (Ensure the same architecture as the saved model)\n",
        "model = RoBERTa_GNN_HAN().to(device)  # Replace with your actual model class\n",
        "\n",
        "# ✅ Load the saved weights\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Model successfully loaded and ready for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSHDbtTV5w-J"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ✅ Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        if len(batch) == 2:\n",
        "            input_ids, labels = batch\n",
        "            subject_embedding = torch.zeros((input_ids.shape[0], 128), device=device)  # Dummy tensor\n",
        "        else:\n",
        "            input_ids, labels, subject_embedding = batch\n",
        "\n",
        "        # ✅ Move to GPU/CPU\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        subject_embedding = subject_embedding.to(device)\n",
        "\n",
        "        # ✅ Forward pass\n",
        "        outputs = model(input_ids, attention_mask=torch.ones_like(input_ids, device=device), subject_embedding=subject_embedding)\n",
        "\n",
        "        # ✅ Convert logits to predicted class\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # ✅ Collect results\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# ✅ Compute Metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
        "precision = precision_score(all_labels, all_preds, average='weighted') * 100\n",
        "recall = recall_score(all_labels, all_preds, average='weighted') * 100\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
        "\n",
        "# ✅ Print Results\n",
        "print(f\"✅ Model Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"✅ Precision: {precision:.2f}%\")\n",
        "print(f\"✅ Recall: {recall:.2f}%\")\n",
        "print(f\"✅ F1-Score: {f1:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UshcHnbo69ZE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# ✅ Load summarization model (BART)\n",
        "summarizer = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# ✅ Load the trained Fake News Detection model\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"  # Update if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Initialize the Fake News Detection Model (Ensure same architecture as saved model)\n",
        "model = RoBERTa_GNN_HAN().to(device)  # Replace with your actual model class\n",
        "\n",
        "# ✅ Load model weights\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Model successfully loaded!\")\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # Update if using a different tokenizer\n",
        "\n",
        "# ✅ Function to summarize the article\n",
        "def summarize_article(article):\n",
        "    inputs = summarizer_tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = summarizer.generate(inputs[\"input_ids\"], max_length=50, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    headline = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return headline\n",
        "\n",
        "# ✅ Function to preprocess and predict\n",
        "def predict_news(model, tokenizer, text, subject):\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    # ✅ Tokenize input text\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # ✅ Encode subject (Modify if using categorical embeddings)\n",
        "    subject_embedding = torch.tensor([subject], dtype=torch.float32).to(device)  # Adjust based on model structure\n",
        "\n",
        "    # ✅ Move inputs to device\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, subject_embedding=subject_embedding)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()  # 0 = Fake, 1 = Real\n",
        "\n",
        "    return \"✅ Real News\" if prediction == 1 else \"❌ Fake News\"\n",
        "\n",
        "# ✅ Example news article\n",
        "news_text = \"\"\"The government has announced a secret plan to replace all paper currency with digital-only transactions overnight. According to anonymous sources, citizens will wake up to find their cash invalid, forcing them to register for a government-controlled financial system. Experts warn this move is a step toward total economic surveillance, though officials have denied such claims.\"\"\"\n",
        "\n",
        "subject_encoded = 3  # Example encoding (Modify based on your model's subject input format)\n",
        "\n",
        "# ✅ Generate a headline\n",
        "headline = summarize_article(news_text)\n",
        "print(\"📰 Summarized Headline:\", headline)\n",
        "\n",
        "# ✅ Predict using summarized headline\n",
        "result = predict_news(model, tokenizer, headline, subject_encoded)\n",
        "print(\"📰 Prediction:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XW-KQZHNKQ_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load pre-trained RoBERTa model for sentiment analysis\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        scores = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(scores).item()\n",
        "\n",
        "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    return labels[predicted_class]\n",
        "\n",
        "# Example usage\n",
        "news_text = \"New Policy Proposal Debated in Parliament\"\n",
        "sentiment = predict_sentiment(news_text)\n",
        "print(f\"Sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW7jBo_uJBnK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ✅ Load summarization model (BART)\n",
        "summarizer = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# ✅ Load the trained Fake News Detection model\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"  # Update if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Initialize Fake News Detection Model (Ensure same architecture)\n",
        "model = RoBERTa_GNN_HAN().to(device)  # Replace with your actual model class\n",
        "\n",
        "# ✅ Load model weights\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Fake News Model Loaded Successfully!\")\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # Update if using a different tokenizer\n",
        "\n",
        "# ✅ Load Sentiment Analysis model (RoBERTa Sentiment)\n",
        "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
        "sentiment_model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Sentiment Analysis Model Loaded Successfully!\")\n",
        "\n",
        "# ✅ Function to summarize the article\n",
        "def summarize_article(article):\n",
        "    inputs = summarizer_tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = summarizer.generate(inputs[\"input_ids\"], max_length=50, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    headline = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return headline\n",
        "\n",
        "# ✅ Function to predict Fake/Real news\n",
        "def predict_news(model, tokenizer, text, subject):\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    # ✅ Tokenize input text\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # ✅ Encode subject properly (Modify if using categorical embeddings)\n",
        "    subject_embedding = torch.tensor([subject], dtype=torch.float32).to(device)\n",
        "\n",
        "    # ✅ Move inputs to device\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, subject_embedding=subject_embedding)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()  # 0 = Fake, 1 = Real\n",
        "\n",
        "    return \"✅ Real News\" if prediction == 1 else \"❌ Fake News\"\n",
        "\n",
        "# ✅ Function to predict sentiment (Positive/Neutral/Negative)\n",
        "def predict_sentiment(text):\n",
        "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "        scores = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(scores).item()\n",
        "\n",
        "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    return labels[predicted_class]\n",
        "\n",
        "# ✅ Example news article\n",
        "news_text = \"\"\"The U.S. Senate has passed a bipartisan bill aimed at strengthening election\n",
        " security ahead of the upcoming presidential elections. The legislation, which received\n",
        " support from both parties, allocates additional funding for cybersecurity measures and\n",
        " voter protection. Lawmakers emphasized the need to safeguard democracy against foreign\n",
        "  interference and misinformation. The bill now moves to the House of Representatives for final approval.\"\"\"\n",
        "\n",
        "subject_encoded = 3  # Example encoding (Modify based on your model's subject input format)\n",
        "\n",
        "# ✅ Generate a headline\n",
        "headline = summarize_article(news_text)\n",
        "print(\"📰 Summarized Headline:\", headline)\n",
        "\n",
        "# ✅ Predict Fake/Real News using summarized headline\n",
        "news_prediction = predict_news(model, tokenizer, headline, subject_encoded)\n",
        "print(\"📰 Fake News Prediction:\", news_prediction)\n",
        "\n",
        "# ✅ Predict sentiment of the news\n",
        "news_sentiment = predict_sentiment(headline)\n",
        "print(\"📊 Sentiment Analysis:\", news_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYmHgRjYFKHT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ✅ Load the trained Fake News Detection model\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"  # Update if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Initialize Fake News Detection Model (Ensure same architecture)\n",
        "model = RoBERTa_GNN_HAN().to(device)  # Replace with your actual model class\n",
        "\n",
        "# ✅ Load model weights\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "print(\"✅ Fake News Model Loaded Successfully!\")\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # Update if using a different tokenizer\n",
        "\n",
        "# ✅ Function to predict Fake/Real news\n",
        "def predict_news(model, tokenizer, text, subject):\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    # ✅ Tokenize input text\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # ✅ Encode subject properly (Modify if using categorical embeddings)\n",
        "    subject_embedding = torch.tensor([subject], dtype=torch.float32).to(device)\n",
        "\n",
        "    # ✅ Move inputs to device\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, subject_embedding=subject_embedding)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()  # 0 = Fake, 1 = Real\n",
        "\n",
        "    return \"✅ Real News\" if prediction == 1 else \"❌ Fake News\"\n",
        "\n",
        "# ✅ Example news article\n",
        "news_text = \"\"\"Politician Admits to Controlling the Weather with Hidden Technology.\"\"\"\n",
        "\n",
        "subject_encoded = 3\n",
        "\n",
        "# ✅ Predict Fake/Real News\n",
        "news_prediction = predict_news(model, tokenizer, news_text, subject_encoded)\n",
        "print(\"📰 Fake News Prediction:\", news_prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbBFZ1wvMyrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c04e7c-c705-450e-a9b6-359ecdbbda67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np4EodyALKgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795b8477-c618-43ec-ec09-a56aa2109015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=454aa71e9259d47c74e2bb6ba33f3282c7b8fc5dd90306006caef06148b0bc55\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.39 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.76.2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.13.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flask transformers googletrans==4.0.0-rc1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQIAjtwxOv8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27658a63-e531-4d21-99f0-d7508e6eaf62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ngrok: command not found\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ngrok config add-authtoken 2tFeAis3sCd5sYSkXCGiAXey4QD_7MuDVoGFZb9LDTrct7MX5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sncs8F4rSBv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80dbf2a-92ad-46ad-8ebf-b4d6b58263c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /root/.config/ngrok/ngrok.yml: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat /root/.config/ngrok/ngrok.yml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5EjW_3jfefC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d207786-fcf5-472b-fe2e-b28be5bfab4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModelForSequenceClassification, RobertaTokenizer\n",
        "from googletrans import Translator\n",
        "from flask import Flask, render_template, request\n",
        "from pyngrok import ngrok  # ✅ For Colab Hosting\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ✅ Define RoBERTa + GNN + HAN Model\n",
        "class RoBERTa_GNN_HAN(nn.Module):\n",
        "    def __init__(self, roberta_model_name=\"roberta-base\", hidden_dim=128, num_classes=2):\n",
        "        super(RoBERTa_GNN_HAN, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
        "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_dim)\n",
        "        self.gnn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subject_embedding):\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = roberta_output.last_hidden_state[:, 0, :]\n",
        "        x = self.relu(self.fc1(hidden_states))\n",
        "        x = self.relu(self.gnn(x + subject_embedding))\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = x * attention_weights\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ✅ Load Models\n",
        "summarizer = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(f\"🚨 Model file not found: {checkpoint_path}\")\n",
        "    model = None\n",
        "else:\n",
        "    try:\n",
        "        model = RoBERTa_GNN_HAN().to(device)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        model.eval()\n",
        "        print(\"✅ Fake News Model Loaded Successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"🚨 Error loading Fake News Model: {e}\")\n",
        "        model = None\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
        "sentiment_model.eval()\n",
        "translator = Translator()\n",
        "\n",
        "def translate_to_english(text, src_language):\n",
        "    try:\n",
        "        return translator.translate(text, src=src_language, dest=\"en\").text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation Error: {e}\")\n",
        "        return text\n",
        "\n",
        "def summarize_article(article):\n",
        "    inputs = summarizer_tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = summarizer.generate(inputs[\"input_ids\"], max_length=50, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def predict_news(model, tokenizer, text, subject=3):\n",
        "    if model is None:\n",
        "        return \"❌ Fake News (Model Not Loaded)\"\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    subject_embedding = torch.tensor([[subject]], dtype=torch.float32).to(device)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, subject_embedding=subject_embedding)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()\n",
        "    return \"✅ Real News\" if prediction == 1 else \"❌ Fake News\"\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "        scores = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(scores).item()\n",
        "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    return labels[predicted_class]\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return render_template(\"home.html\")\n",
        "\n",
        "@app.route(\"/templates\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    if request.method == \"POST\":\n",
        "        user_text = request.form.get(\"news_text\", \"\").strip()\n",
        "        selected_language = request.form.get(\"language\", \"en\")\n",
        "\n",
        "        if not user_text:\n",
        "            return render_template(\"index.html\", error=\"❌ Please enter news text.\", selected_language=selected_language)\n",
        "\n",
        "        translated_text = translate_to_english(user_text, selected_language) if selected_language != \"en\" else user_text\n",
        "        sentiment = predict_sentiment(translated_text)\n",
        "        summarized_text = summarize_article(translated_text)\n",
        "        fake_news_prediction = predict_news(model, tokenizer, summarized_text)\n",
        "\n",
        "        google_search_url = \"\"\n",
        "        if fake_news_prediction == \"✅ Real News\":\n",
        "            query = summarized_text.replace(\" \", \"+\")\n",
        "            google_search_url = f\"https://www.google.com/search?q={query}\"\n",
        "\n",
        "        return render_template(\n",
        "            \"index.html\",\n",
        "            user_text=user_text,\n",
        "            selected_language=selected_language,\n",
        "            translated_text=translated_text,\n",
        "            summarized_text=summarized_text,\n",
        "            fake_news_prediction=fake_news_prediction,\n",
        "            sentiment=sentiment,\n",
        "            google_search_url=google_search_url\n",
        "        )\n",
        "\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"🔗 Public URL: {public_url}\")\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "hRRHfaf-wuer",
        "outputId": "bb22d4a5-fdfc-4832-86f5-42f6562e507f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fake News Model Loaded Successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-24T07:24:57+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n",
            "ERROR:pyngrok.process.ngrok:t=2025-04-24T07:24:57+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-04-24T07:24:57+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-04-24T07:24:57+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8152c2dc6fd0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔗 Public URL: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    429\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUvLwHfsk0Cw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "26ee1cbc-a569-4f67-8d43-a2313553cd3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4700c6a2aa8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m abs = _make_elementwise_unary_prim(\n\u001b[0m\u001b[1;32m    525\u001b[0m     \u001b[0;34m\"abs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0mimpl_aten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m_make_elementwise_unary_prim\u001b[0;34m(name, type_promotion, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \"\"\"\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     return _make_prim(\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{name}(Tensor self) -> Tensor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prim_elementwise_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_promotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_promotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m_make_prim\u001b[0;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         ]\n\u001b[0;32m--> 320\u001b[0;31m         prim_def = torch.library.custom_op(\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;34m\"prims::\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0m_prim_impl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36mcustom_op\u001b[0;34m(name, fn, mutates_args, device_types, schema)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"::\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomOpDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# Check that schema's alias annotations match those of `mutates_args`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, namespace, name, schema, fn)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_library_allowing_overwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_to_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disabled_kernel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mOPDEFS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qualname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/custom_ops.py\u001b[0m in \u001b[0;36m_register_to_dispatcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abstract_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mautograd_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_autograd_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opoverload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_stacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mcaller_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/utils.py\u001b[0m in \u001b[0;36mget_source\u001b[0;34m(stacklevel)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{frame.filename}:{frame.lineno}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    992\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetabsfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;31m# Always map to the name the module knows itself by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             modulesbyfile[f] = modulesbyfile[\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetabsfile\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    961\u001b[0m     normalizes the result as much as possible.\"\"\"\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormcase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    943\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModelForSequenceClassification, RobertaTokenizer\n",
        "from googletrans import Translator\n",
        "from flask import Flask, render_template, request\n",
        "from pyngrok import ngrok  # ✅ For Colab Hosting\n",
        "import os\n",
        "\n",
        "# ✅ Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# ✅ Define RoBERTa + GNN + HAN Model\n",
        "class RoBERTa_GNN_HAN(nn.Module):\n",
        "    def __init__(self, roberta_model_name=\"roberta-base\", hidden_dim=128, num_classes=2):\n",
        "        super(RoBERTa_GNN_HAN, self).__init__()\n",
        "\n",
        "        # ✅ Load pre-trained RoBERTa\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
        "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_dim)\n",
        "\n",
        "        # ✅ GNN Layer - Modeling \"Statement vs Subject\" Relationship\n",
        "        self.gnn = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # ✅ HAN Layer - Attention over Subject Context\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # ✅ Output Layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subject_embedding):\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = roberta_output.last_hidden_state[:, 0, :]  # CLS token representation\n",
        "\n",
        "        # Feature Extraction\n",
        "        x = self.relu(self.fc1(hidden_states))\n",
        "\n",
        "        # GNN Layer - Incorporate Subject Information\n",
        "        x = self.relu(self.gnn(x + subject_embedding))\n",
        "\n",
        "        # HAN - Attention Mechanism\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = x * attention_weights\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ✅ Load summarization model (BART)\n",
        "summarizer = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# ✅ Load Fake News Detection model\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/Checkpoints/best_model.pt\"  # Update path if needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ Check if Model File Exists\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(f\"🚨 Model file not found: {checkpoint_path}\")\n",
        "    model = None\n",
        "else:\n",
        "    try:\n",
        "        model = RoBERTa_GNN_HAN().to(device)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        model.eval()\n",
        "        print(\"✅ Fake News Model Loaded Successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"🚨 Error loading Fake News Model: {e}\")\n",
        "        model = None\n",
        "\n",
        "# ✅ Load tokenizer for Fake News Model\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# ✅ Load sentiment analysis model (RoBERTa)\n",
        "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
        "sentiment_model.eval()\n",
        "\n",
        "# ✅ Load Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "# ✅ Function to translate text to English\n",
        "def translate_to_english(text, src_language):\n",
        "    try:\n",
        "        return translator.translate(text, src=src_language, dest=\"en\").text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation Error: {e}\")\n",
        "        return text  # If translation fails, return original text\n",
        "\n",
        "# ✅ Function to summarize the article\n",
        "def summarize_article(article):\n",
        "    inputs = summarizer_tokenizer(article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = summarizer.generate(inputs[\"input_ids\"], max_length=50, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ✅ Function to predict Fake/Real news\n",
        "def predict_news(model, tokenizer, text, subject=3):\n",
        "    if model is None:\n",
        "        return \"❌ Fake News (Model Not Loaded)\"\n",
        "\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    subject_embedding = torch.tensor([[subject]], dtype=torch.float32).to(device)\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, subject_embedding=subject_embedding)\n",
        "        prediction = torch.argmax(outputs, dim=1).item()  # 0 = Fake, 1 = Real\n",
        "\n",
        "    return \"✅ Real News\" if prediction == 1 else \"❌ Fake News\"\n",
        "\n",
        "# ✅ Function to predict sentiment (Positive/Neutral/Negative)\n",
        "def predict_sentiment(text):\n",
        "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "        scores = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(scores).item()\n",
        "\n",
        "    labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    return labels[predicted_class]\n",
        "\n",
        "# ✅ Flask Routes\n",
        "@app.route(\"/\", methods=[\"GET\"])  # Only allow GET for home page\n",
        "def home():\n",
        "    return render_template(\"home.html\")\n",
        "\n",
        "@app.route(\"/templates\", methods=[\"GET\", \"POST\"])  # Allow POST for analysis\n",
        "def index():\n",
        "    if request.method == \"POST\":\n",
        "        user_text = request.form.get(\"news_text\", \"\").strip()\n",
        "        selected_language = request.form.get(\"language\", \"en\")\n",
        "\n",
        "        if not user_text:\n",
        "            return render_template(\"index.html\", error=\"❌ Please enter news text.\", selected_language=selected_language)\n",
        "\n",
        "        # ✅ Sentiment Analysis Logic\n",
        "        if selected_language == \"en\":\n",
        "            sentiment = predict_sentiment(user_text)  # Analyze sentiment on original English text\n",
        "            translated_text = user_text  # No translation needed\n",
        "        else:\n",
        "            translated_text = translate_to_english(user_text, selected_language)\n",
        "            sentiment = predict_sentiment(translated_text)  # Analyze sentiment on translated text\n",
        "\n",
        "        # ✅ Summarization & Fake News Detection\n",
        "        summarized_text = summarize_article(translated_text)\n",
        "        fake_news_prediction = predict_news(model, tokenizer, summarized_text)\n",
        "\n",
        "        return render_template(\n",
        "            \"index.html\",\n",
        "            user_text=user_text,\n",
        "            selected_language=selected_language,\n",
        "            translated_text=translated_text,\n",
        "            summarized_text=summarized_text,\n",
        "            fake_news_prediction=fake_news_prediction,\n",
        "            sentiment=sentiment\n",
        "        )\n",
        "\n",
        "    return render_template(\"index.html\")  # Default page when accessed via GET\n",
        "# ✅ Run Flask App with Public URL in Google Colab\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"🔗 Public URL: {public_url}\")\n",
        "    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_GiQqRhJn0Q"
      },
      "outputs": [],
      "source": [
        "!ngrok config check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrU348Sxjrfr"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/Project/templates\" \"/content/templates\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p26Wk2RtEqw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838746e9-02be-4174-a1dd-57e021a4665c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "home.html  index.html\n"
          ]
        }
      ],
      "source": [
        "!ls /content/templates\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkWyfGRAREuR",
        "outputId": "960ca982-bda1-4dfb-b0c1-0ad025c07e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-telegram-bot\n",
            "  Downloading python_telegram_bot-22.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (4.13.2)\n",
            "Downloading python_telegram_bot-22.0-py3-none-any.whl (673 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/673.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.5/673.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-telegram-bot\n",
            "Successfully installed python-telegram-bot-22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-telegram-bot requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5v4OpoARIkT",
        "outputId": "4ee3a5ef-cc86-4e1e-d63c-4577961fc536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.11/dist-packages (22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot) (0.28.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOTbRM5ejAe3",
        "outputId": "86742764-2783-4610-efc1-e6c1ea8fe4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.11/dist-packages (22.0)\n",
            "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import telegram\n",
        "import requests\n",
        "from telegram import Update\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters\n",
        "\n",
        "# Your Telegram Bot Token\n",
        "TOKEN = '7882086241:AAEzpPtVVf8ELodur-_QvfFvO5mocgROfD8'\n",
        "bot = telegram.Bot(token=TOKEN)\n",
        "\n",
        "# Flask API URL (corrected URL format)\n",
        "FLASK_URL = \"https://f7f6-34-138-243-104.ngrok-free.app/api/analyze\"  # Corrected URL\n",
        "\n",
        "# Function to handle the command and process news\n",
        "async def process_news(update: Update, context):\n",
        "    # Retrieve the message text (news article)\n",
        "    user_text = update.message.text\n",
        "\n",
        "    # Get user's preferred language (or default to 'en')\n",
        "    selected_language = \"en\"  # Example, modify based on your bot's language feature\n",
        "\n",
        "    # Prepare the data to send to Flask API\n",
        "    data = {\n",
        "        \"news_text\": user_text,\n",
        "        \"language\": selected_language\n",
        "    }\n",
        "\n",
        "    # Send POST request to Flask API\n",
        "    try:\n",
        "        response = requests.post(FLASK_URL, json=data)\n",
        "        response.raise_for_status()  # Will raise an HTTPError if the response code was 4xx/5xx\n",
        "\n",
        "        # Parse the JSON response from Flask\n",
        "        result = response.json()\n",
        "\n",
        "        if \"error\" in result:\n",
        "            await update.message.reply_text(result[\"error\"])\n",
        "        else:\n",
        "            # Display results (summarized text, fake news prediction, sentiment, etc.)\n",
        "            await update.message.reply_text(f\"Sentiment: {result['sentiment']}\\nFake News Prediction: {result['fake_news_prediction']}\\nSummary: {result['summarized_text']}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        await update.message.reply_text(f\"Error communicating with the Flask server: {str(e)}\")\n",
        "\n",
        "# Add your bot's handler and run it\n",
        "async def start(update: Update, context):\n",
        "    await update.message.reply_text('Send me some news text, and I will analyze it!')\n",
        "\n",
        "# Use the Application class instead of Updater\n",
        "application = Application.builder().token(TOKEN).build()\n",
        "\n",
        "# Add handlers\n",
        "application.add_handler(CommandHandler('start', start))\n",
        "application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_news))\n",
        "\n",
        "# Start polling for updates\n",
        "application.run_polling()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "NtCU6F1efLgN",
        "outputId": "e25b2d86-dc81-409a-bbae-6aca5ff19e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot close a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-3db2722e3893>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Start polling for updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mapplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    833\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    834\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, bootstrap_retries, close_loop)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     def create_task(\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwG8OaWRTm7l",
        "outputId": "58301174-c5d8-4b2e-bc14-f2fc75ca5bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot\n",
        "!pip install flask\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYf7M5SfU6RW",
        "outputId": "ad2dfff4-9873-4b8b-ed7c-30fd04601ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.11/dist-packages (22.0)\n",
            "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.11/dist-packages (from python-telegram-bot) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx~=0.27->python-telegram-bot) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (4.13.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}